# Sortons au frais

---

# Table des mati√®res

- [√Ä propos](#√†-propos)
- [Pr√©-requis](#pr√©-requis)
- [Application web](#application-web)
    - [Backend](#backend)
    - [Frontend](#frontend)
- [Analyse statistique : pond√©ration du r√©seau pi√©ton](#analyse-statistique--pond√©ration-du-r√©seau-pi√©ton)


# üöÄ D√©marrage rapide

copier le fichier **.env.example** et le renommer en **.env** √† la racine du projet. 
```bash
cp .env.example .env
```

t√©l√©charger les data de opendata lyon
```bash
cd score_calculation_it/input_data
pip i geopandas owslib
python fetch_data.py
# Selectionner l'option WEB_ONLY
```

t√©l√©charger le r√©seau final pr√©-calcul√©
```bash
mkdir -p score_calculation_it/output_data/network/graph
cd score_calculation_it/output_data/network/graph
wget https://endpoint-minio.projets.erasme.org/fichiers-publics/data_sortons_au_frais/final_network_P0_01O5At0_01Ar10C0_01E5Ca.gpkg
```

lancer le backend et le frontend
```bash
docker-compose up
```

Le frontend est accessible √† l'adresse [http://localhost:3000](http://localhost:3000)
Le backend est accessible √† l'adresse [http://localhost:3002](http://localhost:3002)



# √Ä propos

Le projet Sortons au frais est un projet men√© par Erasme (laboratoire d'innovation de la M√©tropole de Lyon), le service Donn√©es M√©tropolitaines et le service G√©omatique de la M√©tropole de Lyon. Ce projet a √©t√© r√©alis√© en partenariat avec les services G√©omatiques d'une quinzaine de communes de la m√©tropole de Lyon dans le cadre de la [DatAgora](https://datagora.erasme.org/). 
L'objectif est d'apporter une solution d'adaptation √† la canicule en proposant une application avec 3 fonctionnalit√©es principales : 

- Proposer des itin√©raires pi√©tons permettant de se d√©placer "au frais", en minimisant la chaleur le long du trajet
- Trouver les lieux "frais" autour de chez soi
- Afficher des √©l√©ments utiles en cas de canicule (fontaines, parcs, toilettes etc.)

La totalit√© du code et de l'analyse statistique a √©t√© r√©alis√©e dans le cadre d'un stage de master 2 par [Yannis BARBA](https://www.linkedin.com/in/yannis-barba-90b9391bb/) et a donn√© lieu √† un rapport de stage expliquant la d√©marche suivie pour d√©terminer la pond√©ration du r√©seau pi√©ton.
La partie statistique est pr√©sente dans le backend de l'application web mais le d√©tail de l'arborescence des scripts est expliqu√© dans la partie [**Analyse statistique**](#analyse-statistique--pond√©ration-du-r√©seau-pi√©ton).

Toutes les informations du projet se trouvent sur la page [https://datagora.erasme.org/projets/sortons-au-frais/](https://datagora.erasme.org/projets/sortons-au-frais/)

# Pr√©-requis et Installation

Le projet a √©t√© d√©velopp√© dans un environnement conda, si l'application n'est pas ex√©cut√©e via les images dockers, il est pr√©f√©rable de cr√©er un environnement conda. Les versions utilis√©es dans le cadre de ce projet sont les suivantes : 

- **conda** : 23.1.0
- **docker** :  23.0.2
- **docker-compose** : 1.29.2

Une fois le projet t√©l√©charg√© via github, 
- se placer √† la racine du projet et cr√©er un fichier **.env** avec la variable d'environnement
```txt
REACT_APP_URL_SERVER=http://localhost:3002
**REACT_APP_PORT_SERVER**=3002
```
- se placer √† la racine du dossier backend et cr√©er un fichier **.env** avec la variable d'environnement : 
```txt
PORT=3000
```
- se placer √† la racine du dossier frontend et cr√©er un fichier **.env** avec ces variables d'env : 
```txt
REACT_APP_URL_SERVER=http://172.17.0.2:3002
REACT_APP_PORT_SERVER=3002
```
- t√©l√©charger le [r√©seau final](https://endpoint-minio.projets.erasme.org/fichiers-publics/data_sortons_au_frais/final_network_P0_01O5At0_01Ar10C0_01E5Ca.gpkg) et le placer √† cet endroit : *backend/score_calculation_it/output_data/network/graph/final_network_P0_01O5At0_01Ar10C0_01E5Ca.gpkg* 

- t√©l√©charger les donn√©es n√©cessaires au fonctionnement de l'application web en suivant les instructions de la partie [Donn√©es utilisables via l'application Web](#donn√©es-utilisables-via-lapplication-web) et en choisissant l'option "WEB_ONLY".

## Ex√©cution via Docker

Si besoin,les images docker du frontend et du backend sont disponibles √† aux adresses suivantes : 
- backend : https://hub.docker.com/repository/docker/yannisbarba/itineraires_fraicheur_backend/general
- frontend : https://hub.docker.com/repository/docker/yannisbarba/itineraires_fraicheur_frontend/general

Celles-ci peuvent √™tre build en ex√©cutant le fichier **docker-compose.yml**. 
Dans un premier temps, build le backend avec la commande :

```bash
docker-compose build backend
```

Une fois le build r√©alis√©, lancer le container backend avec la commande suivante : 

```bash
docker run yannisbarba/itineraires_fraicheur_backend:latest
```
 
V√©rifier l'adresse sur laquelle le serveur s'ex√©cute (*Running on <adresse-serveur> *), puis modifier si besoin le fichier .env √† la racine du frontend et mettre les variales d'environnement suivantes : 

```txt
REACT_APP_URL_SERVER=<adresse-serveur>
REACT_APP_PORT_SERVER=3002
```

lancer alors le build du frontend via 
```bash
docker-compose build frontend
```

puis ex√©cuter le front end  : 

```bash
docker run yannisbarba/itineraires_fraicheur_frontend:latest
```

## Ex√©cution via conda (conseill√© pour le d√©veloppement)

### Cr√©ation de l'environnement conda

Une fois conda install√© (via anaconda par exemple), se placer √† la racine du projet et cr√©er un environnement conda pour le projet via la commande suivante : 

```bash
conda create --name <nom-env>
```
Suivre les indications de cr√©ations de l'environnement puis une fois √† la racine du projet, activer l'environnement conda : 

```bash
conda activate <nom-env>
```

Puis installer toutes les d√©pendances avec 

```bash
pip install -r requirements.txt
```

## Ex√©cution du backend
**Se placer √† la racine du dossier backend**

Cr√©er un fichier **.env** avec la variable suivante :

```txt
PORT=3000
```

Avant de lancer le backend il est n√©cessaire de t√©l√©charger certaines donn√©es n√©cessaires au bon fonctionnement de l'application, suivre les indications de la partie [Donn√©es utilisables via l'application Web](#donn√©es-utilisables-via-lapplication-web).

Afin de lancer le backend, se positionner √† la racine du dossier backend et ex√©cuter la commande suivante : 

```bash
python app.py
```

## Ex√©cution du frontend
**Se placer √† la racine du dossier frontend**
Cr√©er un fichier **.env** avec les variables suivantes: 

```txt
REACT_APP_URL_SERVER=http://localhost:3002
REACT_APP_PORT_SERVER=3002
```

Avant de lancer l'ex√©cution du frontend, il est n√©cessaire d'installer les d√©pendances npm via la commande : 

```bash
npm install
```

Afin de lancer le frontend, ex√©cuter la commande suivante : 

```bash
npm start
```



# Application web

## Backend


Toutes les variables globales (les chemins des fichiers, les param√®tres sp√©cifiques etc.) sont stock√©es dans le fichier **global_variable.py** √† la racine du dossier backend. Il n'y a pas de base de donn√©es pour ce projet car il n'y en avait pas le besoin. Les quelques informations n√©cessaires au bon fonctionnement du frontend (chemin des layers, chemin des icons etc.) sont √©galement stock√©s directement dans un dictionnaire python dans le fichier **global_variable.py**.

Le backend est structur√© en deux parties principales, l'API d√©velopp√©e avec le framework **Flask** et les donn√©es avec l'analyse statistique (d√©taill√©e [ici](#analyse-statistique--pond√©ration-du-r√©seau-pi√©ton)).

### LES DONN√âES 
L'ensemble des donn√©es utiles pour l'application web (et pour le calcul de score) sont stock√©es dans le dossier *score_calculation_it/input_data*

On peut distinguer deux types de donn√©es : 

- Les [donn√©es](#requ√™te-wfs) directement issues d'une requ√™te WFS √† l'api de datagrandlyon
- Les donn√©es de plus grosse taille et n√©cessitant des calculs sp√©cifiques

#### Donn√©es utilisables via l'application Web

Les donn√©es issues d'une requ√™te WFS √† datagrandlyon peuvent √™tre t√©l√©charg√©es directement en ex√©cutant le script **fetch_data.py** situ√© dans le dossier *score_calculation_it/input_data/*. Il est possible de t√©l√©charger une donn√©e en particulier ou toutes les donn√©es d'un coup.  Afin de lancer le t√©l√©chargement, ex√©cuter le script dans un terminal **en se pla√ßant au niveau du script** puis lancer la commande suivante : 

```bash
python fetch_data.py
```
Puis se laisser guider par les indications dans le terminal.

√Ä chaque fois qu'une donn√©e est t√©l√©charg√©e, un dossier se cr√©√© avec la donn√©e sous format geojson (pour l'affichage sur la web app) et sous format gpkg (pour l'ensemble des calculs). 

### L'API
Le script python principal du backend est le fichier **app.py** qui constitue le *endpoint* du backend permettant d'ex√©cuter les diff√©rentes requ√™tes utilisateur. Avant le lancement de l'application, v√©rifier que le chemin du graph est celui souhait√© dans le fichier **global_variable.py**.

Il y a seulement deux routes:
- requ√™te des layers
- requ√™te pour le calcul d'itin√©raire

Les endpoints se servent de fonction pr√©sentes dans le dossier *models* avec un fichier correspondant √† chaque route.
Les donn√©es distribu√©es pour l'application web sont celles stock√©es dans le dossier *score_calculation_it/input_data/* (cf [partie](#les-donn√©es))


## Frontend
Le frontend est con√ßu en React et react-leaflet pour ce qui est de la cartographie. Il est con√ßu pour √™tre une unique page int√©grable dans un site web. 
Le dossier src contient l'ensemble des scripts avec des composants et un context (permettant la circulation des variables entre les diff√©rents composants).

NB : le fetch des donn√©es *Lieux frais ouverts au public* se fait via une URL temporaire suite √† changement de conception c√¥t√© datagrandlyon. Si modification d'URL il y a, ce changement s'effectue dans le fichier **mainContext.js** Ligne 127.

# Pond√©ration du r√©seau pi√©ton et analyse statistique

## Pre-processing
Afin de pouvoir r√©aliser le calcul de la pond√©ration, il est n√©cessaire de faire un pre-processing. Si les donn√©es sont amen√©es √† √™tre mise √† jour, chaque script peut √™tre ex√©cutable pour relancer les calculs sp√©cifiques √† chaque donn√©e. En sortie de chaque script on obtient un sous-r√©seau enregistr√© dans un fichier **edges_nom_donn√©es.gpkg** qui nous permet d'avoir le taux de recouvrement ou la pr√©sence d'une donn√©e sur chaque segments. Ce sont tous ces fichiers qui sont ensuite utilis√©s pour les calculs de score. Ces sous-r√©seaux suffisent pour le [calcul de score](#pond√©ration-du-graph-calcul-du-score).

Tous ces sous-r√©seaux sont directements disponibles dans le dossier √† cette [adresse](https://minio.projets.erasme.org/browser/fichiers-publics/ZGF0YV9zb3J0b25zX2F1X2ZyYWlzL2VkZ2VzLw==). Placer chaque r√©seau √† l'emplacement suivant : *./score_calculation_it/output_data/network/edges/*.

Ils correpondent √† une version des donn√©es de l'√©t√© 2023. Toutefois, si l'on souhaite recalculer une ou toutes les donn√©es, suivre les instructions pour chaque donn√©es.

### Le r√©seau de la m√©tropole
Cette donn√©e est indispensable pour la suite (√† t√©l√©charger en premier lieu donc). Afin de la mettre √† jour, ex√©cuter le fichier 
**fetch_network.py** √† partir de *./score_calculation_it/input_data/* et se laisser guider par les instructions du terminal.

```bash
python fetch_network.py
```

### Les POIs
Actuellement les points d'inter√™ts (POI) ne sont pas pris en compte dans la pond√©ration du graphe, cependant, il existe un fichier **poi_preprocessing.py** permettant de calculer la pr√©sence de POI sur les segments. Les r√©sultats pourraient √™tre utilis√©s dans le cadre d'une am√©lioration du calculateur d'itin√©raire. 
Afin de lancer les calculs, se placer ici : *./score_calculation_it/* puis ex√©cuter le fichier et se laisser guider par les instructions du temrinal. 

```bash
python poi_preprocessing.py
```
### Parcs et Jardins
Les parcs ont un traitement un peu diff√©rents des autres POI, par cons√©quent, les calculs n√©cessaire pour le calculateur d'itin√©raire peuvent √™tre ex√©cut√© via le fichier **parcs_jardins_preprocessing.py** et en se laissant guider par les instructions du terminal.

```bash
python parcs_jardins_preprocessing.py
```

### Eaux

Les cours d'eau ont un traitement un peu diff√©rents des autres POI, par cons√©quent, les calculs n√©cessaire pour le calculateur d'itin√©raire peuvent √™tre ex√©cut√© via le fichier **eaux_preprocessing.py** et en se laissant guider par les instructions du terminal.

```bash
python eaux_preprocessing.py
```

### La v√©g√©tation
La donn√©e de v√©g√©tation stratifi√©e la donn√©e la plus volumineuse. Il est possible de la recalculer de A √† Z en partant de la donn√©e brute pr√©sente √† [cette addresse](https://data.grandlyon.com/portail/fr/jeux-de-donnees/vegetation-stratifiee-2018-metropole-lyon/telechargements). Afin de pouvoir l'utiliser dans le cadre de ce projet, des calculs ont √©t√© r√©alis√©s avec Qgis.
- R√©duire la r√©solution du raster (de 1m √† 5m). 
- Vectoriser le raster
- r√©duire le nombre de classes (de 5 √† 3). Les nouvelles classes sont : 
    - arbres (>1.5m)
    - arbustes (<1.5m)
    - prairies (anciennement herbac√©es)
Les temps de calculs sont relativement longs et demandent une RAM assez importante (> 16G).
Sauvegarder le r√©sultat sous format Geopackage (gpkg). 

Sinon, la donn√©e d√©j√† calcul√©e est pr√©sente √† [cette adresse](https://endpoint-minio.projets.erasme.org/fichiers-publics/data_sortons_au_frais/raw_veget_strat.gpkg)
Une fois t√©l√©charg√©e, la donn√© de v√©g√©tation doit √™tre sauvegard√©e ici : *"./score_calculation_it/input_data/vegetation/raw_veget_strat.gpkg"*
Cette donn√©e √©tant encore trop volumineuse pour √™tre manipul√©e ais√©ment dans le cadre du projet, elle a √©t√© "clipp√©" avec la version bufferis√©e du r√©seau OSM. Le calcul √©tant long (24h !) avec une configuration standard, la version d√©j√† calcul√©e en date du 03.07.23 est pr√©sente [ici](https://endpoint-minio.projets.erasme.org/fichiers-publics/data_sortons_au_frais/clipped_veget_12.gpkg).
Pour mettre √† jour cette donn√©e, ex√©cuter le script **vegetation_preprocessing.py**. en se pla√ßant ici *./score_calculation_it/* et se laisser guider par les indications du terminal.

```bash
python vegetation_preprocessing.py
```

### La temp√©rature

La donn√©e de temp√©rature est √©galement une donn√©e demandant des pr√©-calculs sp√©cifiques. Le tutoriel pour recalculer cette donn√©e est disponible [ici](https://endpoint-minio.projets.erasme.org/fichiers-publics/data_sortons_au_frais/Mise_a_jour_donnees_temp_surface.pdf).
La donn√©e d√©j√† calcul√©e est disponible √† [cette adresse](https://endpoint-minio.projets.erasme.org/fichiers-publics/data_sortons_au_frais/temperature_surface.gpkg). Une fois t√©l√©charg√©e, elle doit √™tre sauvegard√©e ici : *"./score_calculation_it/input_data/temperature/temperature_surface.gpkg"*.
Pour relancer le calcul de la temp√©rature moyenne par segment, ex√©cuter le script **temperature_preprocessing.py**en se pla√ßant ici *./score_calculation_it/* et suivre les indications du terminal.

```bash
python temperature_preprocessing.py
```
### L'ombre des b√¢timents
La donn√©e b√¢timents est une donn√©e requ√™table en WFS en suivant les instructions de la partie [requete WFS](#requ√™te-wfs). Cependant, c'est la donn√©e d'ombre calcul√©e √† partir de la hauteur des b√¢timents qui est utilis√©e dans le calculateur d'itin√©raires. Tous les r√©sultats des calculs interm√©diaires pour les ombres sont disponibles [ici](https://minio.projets.erasme.org/browser/fichiers-publics/ZGF0YV9zb3J0b25zX2F1X2ZyYWlzL29tYnJlcy8=), ils sont √† placer dans le dossier *score_calculation_it/output_data/ombres/*
Le calcul est ex√©cutable en se pla√ßant ici : *./score_calculation_it/* et en ex√©cutant le fichier **ombre_preprocessing.py** et en se laissant guider par le terminal. 

```bash
python ombre_preprocessing.py
```
Tel que le script est con√ßu aujourd'hui, il n'est utile de mettre √† jour la donn√©e que si la donn√©e des b√¢timents ou le r√©seau pi√©ton est mise √† jour. Le script n'est pas con√ßu pour choisir l'horaire et la date √† laquelle faire le calcul. Cependant, ce script peut √™tre assez facilement g√©n√©ralis√©. 

## Pond√©ration du graph (calcul du score)
La pond√©ration du graph ne peut se faire que si l'ensemble des sous-r√©seaux existent (et ont √©t√© mis √† jour au besoin). La pond√©ration du graph est √† renseigner directement dans le fichier **score_calculation.py** en suivant l'exemple *final_params* puis peut √™tre ex√©cut√© via la commande suivante : 

```bash
python score_calculation.py
```

## Analyse de la pond√©ration
Afin de trouver la meilleure pond√©ration, la m√©thodologie suivie est celle du rapport de M2 disponible [ici](mettrerapport). Les calculs sont effectu√©s √† l'aide des fichiers **score_calculation** et **score_analyse.py**. Puis l'analyse est r√©alis√©e dans le fichier **analyse_pipeline_new.ipynb**. 

Afin de tester plusieurs pond√©rations, il suffit de compl√©ter l'objet *final_params* ou bien cr√©er un nouvel objet param√®tre et lancer de nouveau le script **score_calculation.py**. 

Une fois les calculs de score r√©alis√©s, remplacer l'argument de la fonction *pipeline_generate_dataset_new* du fichier **score_analyse.py** afin de g√©n√©rer des datasets de 1000 itin√©raires.
ATTENTION, pour faire une analyse coh√©rente, il est n√©cessaire de pouvoir comparer les itin√©raires deux √† deux donc en conservant les m√™mes noeuds. Les fichiers d√©j√† g√©n√©r√©s sont disponibles [ici](mettreliens). IL est toutefois possible de reg√©n√©rer des noeuds al√©atoires avec la fonctions *create_random_nodes*.
Une fois t√©l√©charg√©s les noeuds sont √† mettre dans le dossier *./score_calculation_it/output_data/analyse* sous les noms : 
- selected_end_nodes.gpkg
- selected_start_nodes.gpkg

Il faut compter environ 1h20 de calcul pour obtenir le graphe pond√©r√© et les 1000 itin√©raires pour une pond√©ration donn√©e √† une heure donn√©e. Lors de l'ex√©cution du script **score_analyse.py**, un dossier se cr√©√© avec tous les fichiers n√©cessaires √† l'analyse du score.

L'analyse a ensuite √©t√© r√©alis√©e √† partir du fichier **analyse_pipeline_new.py**. Les fichiers suivants sont des test / brouillons conserv√©s pour la trace de la r√©flexion : 
- **acp.py** et **acp.ipynb**
- **score_analyse_pipeline.ipynb**


<!-- Security scan triggered at 2025-09-02 00:09:56 -->